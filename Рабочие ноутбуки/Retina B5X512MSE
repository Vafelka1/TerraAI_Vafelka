{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1miNZ2XWacyBQ36RzI8Vo83y9eAPViwNd","timestamp":1716739164386},{"file_id":"18qVX4QOORiqYod8ftEgI-CBwpOzoBDgK","timestamp":1715330961993},{"file_id":"1-3ztSCtqkrUKBGXR-Efd2eVgDA7krrz7","timestamp":1715278672676}],"machine_shape":"hm","authorship_tag":"ABX9TyM1ZfzZIbaatgEAWN/vp+de"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import gdown\n","url = 'https://drive.google.com/drive/folders/1QoCcwlphHKGZMLl_s7wrRjc36_MkCtj5?usp=sharing'\n","gdown.download_folder(url, quiet=True)\n","!unzip -q '/content/Data/KaggleOnly.zip' -d '/content'\n","\n","import shutil\n","shutil.copy('/content/Data/trainLabels.csv', '/content/trainLabels.csv')\n","shutil.copy('/content/Data/valLabels.csv', '/content/valLabels.csv')\n","shutil.copy('/content/Data/testLabels.csv', '/content/testLabels.csv')"],"metadata":{"id":"jfZBVWILYSZa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9f65f6bd-3197-432d-c353-84f2d5c7b3b3","executionInfo":{"status":"ok","timestamp":1717017902547,"user_tz":-180,"elapsed":49959,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["unzip:  cannot find or open /Data/KaggleOnly.zip, /Data/KaggleOnly.zip.zip or /Data/KaggleOnly.zip.ZIP.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","TRAIN_PATH = '/content/preprocessed/' # Папка для обучающего набора данных\n","NEW_TRAIN_PATH = '/content/Train/'  # Папка для перегона данных\n","# Определение списка имен классов\n","CLASS_LIST = sorted(os.listdir(TRAIN_PATH))\n","# Определение количества классов\n","CLASS_COUNT = len(CLASS_LIST)\n","# Проверка результата\n","print(f'Количество классов: {CLASS_COUNT}, метки классов: {CLASS_LIST}')"],"metadata":{"id":"LXppbpuKZcmL","executionInfo":{"status":"ok","timestamp":1717018086232,"user_tz":-180,"elapsed":3,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"24711461-bc44-4f56-a2c2-f3d00988b778"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Количество классов: 5, метки классов: ['0', '1', '2', '3', '4']\n"]}]},{"cell_type":"code","source":["os.mkdir(NEW_TRAIN_PATH)\n","for class_name in CLASS_LIST:                              # Для всех классов по порядку номеров (их меток)\n","    class_path = f'{TRAIN_PATH}{class_name}'              # Формирование полного пути к папке с изображениями класса\n","    new_train_path = f'{NEW_TRAIN_PATH}'                # Полный путь\n","    class_files = os.listdir(class_path)                   # Получение списка имен файлов с изображениями текущего класса\n","    for f in class_files:                                   # Перемещение тестовых файлов в новую папку\n","      os.rename(f'{class_path}/{f}', f'{new_train_path}{f}')\n","      #print(f'{class_path}/{f}-----{new_train_path}{f}')"],"metadata":{"id":"lqdnfVlPQP95","executionInfo":{"status":"ok","timestamp":1717018117101,"user_tz":-180,"elapsed":832,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(\"We are currently in the folder of \",os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZoXLomQpe_c","executionInfo":{"status":"ok","timestamp":1717018181824,"user_tz":-180,"elapsed":336,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}},"outputId":"6c5a5ac1-d54f-4c5a-8b17-a5cceb1c3a7f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["We are currently in the folder of  /content\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aY4Z-t0ppHQI"},"outputs":[],"source":["import torch\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","LEARNING_RATE = 3e-5\n","WEIGHT_DECAY = 5e-4\n","BATCH_SIZE = 28\n","NUM_EPOCHS = 10\n","NUM_WORKERS = 6\n","CHECKPOINT_FILE = \"##EffNetB3_512_acc79_kap76.pth\"\n","PIN_MEMORY = True\n","SAVE_MODEL = True\n","LOAD_MODEL = True\n","\n","# Data augmentation for images\n","train_transforms = A.Compose(\n","    [\n","        A.Resize(width=512, height=512),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.RandomRotate90(p=0.5),\n","        A.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(0, 0), p=0.5),\n","        A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=True, p=1),\n","        A.CoarseDropout(max_holes=12, max_height=10, max_width=20, p=0.3),\n","        A.Normalize(\n","            mean=[0.3199, 0.2240, 0.1609],\n","            std=[0.3020, 0.2183, 0.1741],\n","            max_pixel_value=255.0,\n","        ),\n","        ToTensorV2(),\n","    ]\n",")\n","\n","val_transforms = A.Compose(\n","    [\n","        A.Resize(height=512, width=512),\n","        A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=True, p=1),\n","        A.Normalize(\n","            mean=[0.3199, 0.2240, 0.1609],\n","            std=[0.3020, 0.2183, 0.1741],\n","            max_pixel_value=255.0,\n","        ),\n","        ToTensorV2(),\n","    ]\n",")"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from tqdm import tqdm\n","\n","\n","class DRDataset(Dataset):\n","    def __init__(self, images_folder, path_to_csv, train=True, transform=None):\n","        super().__init__()\n","        self.data = pd.read_csv(path_to_csv)\n","        self.images_folder = images_folder\n","        self.image_files = os.listdir(images_folder)\n","        self.transform = transform\n","        self.train = train\n","\n","    def __len__(self):\n","        return self.data.shape[0] if self.train else len(self.image_files)\n","\n","    def __getitem__(self, index):\n","        if self.train:\n","            image_file, label = self.data.iloc[index]\n","        else:\n","            # if test simply return -1 for label, I do this in order to\n","            # re-use same dataset class for test set submission later on\n","            image_file, label = self.image_files[index], -1\n","            image_file = image_file.replace(\".jpeg\", \"\")\n","\n","        image = np.array(Image.open(os.path.join(self.images_folder, image_file+\".jpeg\")))\n","\n","        if self.transform:\n","            image = self.transform(image=image)[\"image\"]\n","\n","        return image, label, image_file\n"],"metadata":{"id":"vucZ7ljWpRR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import warnings\n","import torch.nn.functional as F\n","\n","\n","def make_prediction(model, loader, output_csv=\"submission.csv\"):\n","    preds = []\n","    filenames = []\n","    model.eval()\n","\n","    for x, y, files in tqdm(loader):\n","        x = x.to(DEVICE)\n","        with torch.no_grad():\n","            predictions = model(x)\n","            # Convert MSE floats to integer predictions\n","            predictions[predictions < 0.5] = 0\n","            predictions[(predictions >= 0.5) & (predictions < 1.5)] = 1\n","            predictions[(predictions >= 1.5) & (predictions < 2.5)] = 2\n","            predictions[(predictions >= 2.5) & (predictions < 3.5)] = 3\n","            predictions[(predictions >= 3.5) & (predictions < 10000000)] = 4\n","            predictions = predictions.long().squeeze(1)\n","            preds.append(predictions.cpu().numpy())\n","            filenames += files\n","\n","    df = pd.DataFrame({\"image\": filenames, \"level\": np.concatenate(preds, axis=0)})\n","    df.to_csv(output_csv, index=False)\n","    model.train()\n","    print(\"Done with predictions\")\n","\n","\n","def check_accuracy(loader, model, device=\"cuda\"):\n","    model.eval()\n","    all_preds, all_labels = [], []\n","    num_correct = 0\n","    num_samples = 0\n","\n","    for x, y, filename in tqdm(loader):\n","        x = x.to(device=device)\n","        y = y.to(device=device)\n","\n","        with torch.no_grad():\n","            predictions = model(x)\n","\n","        # Convert MSE floats to integer predictions\n","        predictions[predictions < 0.5] = 0\n","        predictions[(predictions >= 0.5) & (predictions < 1.5)] = 1\n","        predictions[(predictions >= 1.5) & (predictions < 2.5)] = 2\n","        predictions[(predictions >= 2.5) & (predictions < 3.5)] = 3\n","        predictions[(predictions >= 3.5) & (predictions < 100)] = 4\n","        predictions = predictions.long().view(-1)\n","        y = y.view(-1)\n","\n","        num_correct += (predictions == y).sum()\n","        num_samples += predictions.shape[0]\n","\n","        # add to lists\n","        all_preds.append(predictions.detach().cpu().numpy())\n","        all_labels.append(y.detach().cpu().numpy())\n","\n","    print(\n","        f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}\"\n","    )\n","    model.train()\n","    return np.concatenate(all_preds, axis=0, dtype=np.int64), np.concatenate(\n","        all_labels, axis=0, dtype=np.int64\n","    )\n","\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer, lr):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    #optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","\n","    # If we don't do this then it will just have learning rate of old checkpoint\n","    # and it will lead to many hours of debugging \\:\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","\n","def get_csv_for_blend(loader, model, output_csv_file):\n","    warnings.warn(\"Important to have shuffle=False (and to ensure batch size is even size) when running get_csv_for_blend also set val_transforms to train_loader!\")\n","    model.eval()\n","    filename_first = []\n","    filename_second = []\n","    labels_first = []\n","    labels_second = []\n","    all_features = []\n","\n","    for idx, (images, y, image_files) in enumerate(tqdm(loader)):\n","        images = images.to(DEVICE)\n","\n","        with torch.no_grad():\n","            features = F.adaptive_avg_pool2d(\n","                model.extract_features(images), output_size=1\n","            )\n","            features_logits = features.reshape(features.shape[0] // 2, 2, features.shape[1])\n","            preds = model(images).reshape(images.shape[0] // 2, 2, 1)\n","            new_features = (\n","                torch.cat([features_logits, preds], dim=2)\n","                .view(preds.shape[0], -1)\n","                .cpu()\n","                .numpy()\n","            )\n","            all_features.append(new_features)\n","            filename_first += image_files[::2]\n","            filename_second += image_files[1::2]\n","            labels_first.append(y[::2].cpu().numpy())\n","            labels_second.append(y[1::2].cpu().numpy())\n","\n","    all_features = np.concatenate(all_features, axis=0)\n","    df = pd.DataFrame(\n","        data=all_features, columns=[f\"f_{idx}\" for idx in range(all_features.shape[1])]\n","    )\n","    df[\"label_first\"] = np.concatenate(labels_first, axis=0)\n","    df[\"label_second\"] = np.concatenate(labels_second, axis=0)\n","    df[\"file_first\"] = filename_first\n","    df[\"file_second\"] = filename_second\n","    df.to_csv(output_csv_file, index=False)\n","    model.train()"],"metadata":{"id":"sAqed-eSpdhr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install efficientnet_pytorch"],"metadata":{"id":"dKjNCp5Emodf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716743225694,"user_tz":-180,"elapsed":30764,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}},"outputId":"a2201f06-2723-4449-869a-e25e67450e51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet_pytorch\n","Successfully installed efficientnet_pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["from efficientnet_pytorch import EfficientNet\n","model = EfficientNet.from_pretrained(\"efficientnet-b5\")\n","print(model)"],"metadata":{"id":"shdstzzjtEoN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716743230494,"user_tz":-180,"elapsed":2715,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}},"outputId":"17a71b0a-62a7-4c71-dfec-d32931d11485"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b5-b6417697.pth\n","100%|██████████| 117M/117M [00:01<00:00, 76.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained weights for efficientnet-b5\n","EfficientNet(\n","  (_conv_stem): Conv2dStaticSamePadding(\n","    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n","    (static_padding): ZeroPad2d((0, 1, 0, 1))\n","  )\n","  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","  (_blocks): ModuleList(\n","    (0): MBConvBlock(\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n","        (static_padding): ZeroPad2d((1, 1, 1, 1))\n","      )\n","      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        48, 12, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        12, 48, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (1-2): 2 x MBConvBlock(\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n","        (static_padding): ZeroPad2d((1, 1, 1, 1))\n","      )\n","      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        24, 6, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        6, 24, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (3): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n","        (static_padding): ZeroPad2d((0, 1, 0, 1))\n","      )\n","      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        144, 6, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        6, 144, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (4-7): 4 x MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        240, 240, kernel_size=(3, 3), stride=(1, 1), groups=240, bias=False\n","        (static_padding): ZeroPad2d((1, 1, 1, 1))\n","      )\n","      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        240, 10, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        10, 240, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (8): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        240, 240, kernel_size=(5, 5), stride=[2, 2], groups=240, bias=False\n","        (static_padding): ZeroPad2d((1, 2, 1, 2))\n","      )\n","      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        240, 10, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        10, 240, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        240, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (9-12): 4 x MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False\n","        (static_padding): ZeroPad2d((2, 2, 2, 2))\n","      )\n","      (_bn1): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        384, 16, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        16, 384, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (13): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        384, 384, kernel_size=(3, 3), stride=[2, 2], groups=384, bias=False\n","        (static_padding): ZeroPad2d((1, 1, 1, 1))\n","      )\n","      (_bn1): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        384, 16, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        16, 384, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(128, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (14-19): 6 x MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(768, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        768, 768, kernel_size=(3, 3), stride=(1, 1), groups=768, bias=False\n","        (static_padding): ZeroPad2d((1, 1, 1, 1))\n","      )\n","      (_bn1): BatchNorm2d(768, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        768, 32, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        32, 768, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(128, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (20): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(768, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        768, 768, kernel_size=(5, 5), stride=[1, 1], groups=768, bias=False\n","        (static_padding): ZeroPad2d((2, 2, 2, 2))\n","      )\n","      (_bn1): BatchNorm2d(768, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        768, 32, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        32, 768, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        768, 176, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(176, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (21-26): 6 x MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1056, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1056, 1056, kernel_size=(5, 5), stride=(1, 1), groups=1056, bias=False\n","        (static_padding): ZeroPad2d((2, 2, 2, 2))\n","      )\n","      (_bn1): BatchNorm2d(1056, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1056, 44, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        44, 1056, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(176, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (27): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1056, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1056, 1056, kernel_size=(5, 5), stride=[2, 2], groups=1056, bias=False\n","        (static_padding): ZeroPad2d((2, 2, 2, 2))\n","      )\n","      (_bn1): BatchNorm2d(1056, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1056, 44, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        44, 1056, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1056, 304, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (28-35): 8 x MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1824, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1824, 1824, kernel_size=(5, 5), stride=(1, 1), groups=1824, bias=False\n","        (static_padding): ZeroPad2d((2, 2, 2, 2))\n","      )\n","      (_bn1): BatchNorm2d(1824, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1824, 76, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        76, 1824, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (36): MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(1824, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        1824, 1824, kernel_size=(3, 3), stride=[1, 1], groups=1824, bias=False\n","        (static_padding): ZeroPad2d((1, 1, 1, 1))\n","      )\n","      (_bn1): BatchNorm2d(1824, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        1824, 76, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        76, 1824, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        1824, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","    (37-38): 2 x MBConvBlock(\n","      (_expand_conv): Conv2dStaticSamePadding(\n","        512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn0): BatchNorm2d(3072, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_depthwise_conv): Conv2dStaticSamePadding(\n","        3072, 3072, kernel_size=(3, 3), stride=(1, 1), groups=3072, bias=False\n","        (static_padding): ZeroPad2d((1, 1, 1, 1))\n","      )\n","      (_bn1): BatchNorm2d(3072, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_se_reduce): Conv2dStaticSamePadding(\n","        3072, 128, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_se_expand): Conv2dStaticSamePadding(\n","        128, 3072, kernel_size=(1, 1), stride=(1, 1)\n","        (static_padding): Identity()\n","      )\n","      (_project_conv): Conv2dStaticSamePadding(\n","        3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (static_padding): Identity()\n","      )\n","      (_bn2): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","      (_swish): MemoryEfficientSwish()\n","    )\n","  )\n","  (_conv_head): Conv2dStaticSamePadding(\n","    512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","    (static_padding): Identity()\n","  )\n","  (_bn1): BatchNorm2d(2048, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n","  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n","  (_dropout): Dropout(p=0.4, inplace=False)\n","  (_fc): Linear(in_features=2048, out_features=1000, bias=True)\n","  (_swish): MemoryEfficientSwish()\n",")\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn, optim\n","import os\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from sklearn.metrics import cohen_kappa_score\n","from efficientnet_pytorch import EfficientNet\n","from torchvision.utils import save_image\n","\n","\n","def train_one_epoch(loader, model, optimizer, loss_fn, scaler, device,save_ex=False):\n","    losses = []\n","    loop = tqdm(loader)\n","    for batch_idx, (data, targets, _) in enumerate(loop):\n","        # save examples and make sure they look ok with the data augmentation,\n","        # tip is to first set mean=[0,0,0], std=[1,1,1] so they look \"normal\"\n","        #if save_ex: save_image(data, f\"hi_{batch_idx}.png\")\n","\n","        data = data.to(device=device)\n","        targets = targets.to(device=device)\n","\n","        # forward\n","        with torch.cuda.amp.autocast():\n","            scores = model(data)\n","            loss = loss_fn(scores, targets.unsqueeze(1).float())\n","\n","        losses.append(loss.item())\n","\n","        # backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        loop.set_postfix(loss=loss.item())\n","\n","    print(f\"Loss average over epoch: {sum(losses)/len(losses)}\")\n","\n","\n","def fit():\n","    train_ds = DRDataset(\n","        images_folder=\"Train\",\n","        path_to_csv=\"trainLabels.csv\",\n","        transform=train_transforms,\n","    )\n","    val_ds = DRDataset(\n","        images_folder=\"Train\",\n","        path_to_csv=\"valLabels.csv\",\n","        transform=val_transforms,\n","    )\n","    test_ds = DRDataset(\n","        images_folder=\"Train\",\n","        path_to_csv=\"testLabels.csv\",\n","        transform=val_transforms,\n","        train=False,\n","    )\n","    test_loader = DataLoader(\n","        test_ds, batch_size=BATCH_SIZE, num_workers=6, shuffle=False\n","    )\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=False,\n","    )\n","    val_loader = DataLoader(\n","        val_ds,\n","        batch_size=BATCH_SIZE,\n","        num_workers=2,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=False,\n","    )\n","    loss_fn = nn.MSELoss()\n","\n","    model = EfficientNet.from_pretrained(\"efficientnet-b5\")\n","    #model._fc = nn.Flatten()\n","    model._fc = nn.Linear(2048, 1)\n","    model = model.to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n","        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n","\n","    # Run after training is done and you've achieved good result\n","    # on validation set, then run train_blend.py file to use information\n","    # about both eyes concatenated\n","    #get_csv_for_blend(val_loader, model, \"../train/val_blend.csv\")\n","    #get_csv_for_blend(train_loader, model, \"../train/train_blend.csv\")\n","    #get_csv_for_blend(test_loader, model, \"../train/test_blend.csv\")\n","    #make_prediction(model, test_loader, \"submission_.csv\")\n","    #import sys\n","    #sys.exit()\n","    #make_prediction(model, test_loader)\n","    print(NUM_EPOCHS , ' эпох')\n","    for epoch in range(NUM_EPOCHS):\n","        print(\"Эпоха: \",epoch)\n","        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, DEVICE)\n","\n","        # get on validation\n","        preds, labels = check_accuracy(val_loader, model, DEVICE)\n","        print(f\"QuadraticWeightedKappa (Validation): {cohen_kappa_score(labels, preds, weights='quadratic')}\")\n","\n","        # get on train\n","        #preds, labels = check_accuracy(train_loader, model, DEVICE)\n","        #print(f\"QuadraticWeightedKappa (Training): {cohen_kappa_score(labels, preds, weights='quadratic')}\")\n","\n","        if SAVE_MODEL:\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","            }\n","            save_checkpoint(checkpoint, filename=f\"b5_ft_{epoch}.pth.tar\")\n"],"metadata":{"id":"d86ZGZ6Opi5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()"],"metadata":{"id":"U-wJ-nTYthd4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fit()"],"metadata":{"id":"UApu3zvrXVMX","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4beb8ea2-21a7-47bd-fb6a-212695f9ecb1","executionInfo":{"status":"error","timestamp":1716747178563,"user_tz":-180,"elapsed":3650895,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained weights for efficientnet-b5\n","10  эпох\n","Эпоха:  0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:20<00:00,  2.67it/s, loss=0.371]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.6325017043564289\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:22<00:00,  5.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2567 / 3514 with accuracy 73.05\n","QuadraticWeightedKappa (Validation): 0.6305807535042924\n","=> Saving checkpoint\n","Эпоха:  1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:19<00:00,  2.68it/s, loss=0.269]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.4441906943726798\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:24<00:00,  5.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2696 / 3514 with accuracy 76.72\n","QuadraticWeightedKappa (Validation): 0.7082956857153515\n","=> Saving checkpoint\n","Эпоха:  2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:20<00:00,  2.67it/s, loss=0.109]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.3865273764527301\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:24<00:00,  5.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2730 / 3514 with accuracy 77.69\n","QuadraticWeightedKappa (Validation): 0.6912074745472618\n","=> Saving checkpoint\n","Эпоха:  3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:20<00:00,  2.68it/s, loss=0.294]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.34553587528800955\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:24<00:00,  5.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2740 / 3514 with accuracy 77.97\n","QuadraticWeightedKappa (Validation): 0.7238360510722914\n","=> Saving checkpoint\n","Эпоха:  4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:19<00:00,  2.68it/s, loss=0.0285]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.30840625076929257\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:24<00:00,  5.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2782 / 3514 with accuracy 79.17\n","QuadraticWeightedKappa (Validation): 0.7463973646244588\n","=> Saving checkpoint\n","Эпоха:  5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:19<00:00,  2.68it/s, loss=0.152]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.2881094367301042\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:24<00:00,  5.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2757 / 3514 with accuracy 78.46\n","QuadraticWeightedKappa (Validation): 0.7672287311102977\n","=> Saving checkpoint\n","Эпоха:  6\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:20<00:00,  2.68it/s, loss=0.201]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.2671582605616504\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:25<00:00,  5.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2812 / 3514 with accuracy 80.02\n","QuadraticWeightedKappa (Validation): 0.7582856912473888\n","=> Saving checkpoint\n","Эпоха:  7\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:20<00:00,  2.68it/s, loss=0.00043]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.254833654157365\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:24<00:00,  5.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2802 / 3514 with accuracy 79.74\n","QuadraticWeightedKappa (Validation): 0.756234786724636\n","=> Saving checkpoint\n","Эпоха:  8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1017/1017 [06:19<00:00,  2.68it/s, loss=0.0074]\n"]},{"output_type":"stream","name":"stdout","text":["Loss average over epoch: 0.23589193439678882\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 126/126 [00:25<00:00,  4.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got 2805 / 3514 with accuracy 79.82\n","QuadraticWeightedKappa (Validation): 0.767105315726075\n","=> Saving checkpoint\n","Эпоха:  9\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 5/1017 [00:03<12:21,  1.37it/s, loss=0.219]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-2e8ead0159f0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-f3923c6ad5c0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Эпоха: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# get on validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-f3923c6ad5c0>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, scaler, device, save_ex)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_avg_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# scale drop connect_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Pointwise Convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_project_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["train_ds1 = DRDataset(\n","        images_folder=\"Train\",\n","        path_to_csv=\"trainLabels.csv\",\n","        transform=train_transforms,\n","    )\n","train_loader = DataLoader(\n","        train_ds1,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=False,\n","    )"],"metadata":{"id":"auVfEQ9uRqt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs, classes, _ = next(iter(train_loader))\n","save_image(inputs, f\"hi_0.png\")"],"metadata":{"id":"-M4SdOb0Ri7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","Image.open(\"hi_0.png\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":658,"output_embedded_package_id":"1GOrHVdnEXzbNpXk3TY8vzyHpefiU7up8"},"id":"6IEgCQBMTeFe","executionInfo":{"status":"ok","timestamp":1716747196066,"user_tz":-180,"elapsed":6577,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}},"outputId":"28ac23aa-84c0-4992-f9fb-8ab291c07830"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import shutil\n","shutil.copy('/content/b5_ft_7.pth.tar', '/content/drive/MyDrive/Диплом/Models/EffNetB5_512acc77kap761.pth.tar')"],"metadata":{"id":"_QDyhTQjU0mP","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1716743290456,"user_tz":-180,"elapsed":5920,"user":{"displayName":"Vafelka","userId":"11276804272897708630"}},"outputId":"f217d450-d4bc-49fc-a841-73023c43f000"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/EffNetB5_512acc77kap761.pth.tar'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[],"metadata":{"id":"-8DRlmK7ehuH"},"execution_count":null,"outputs":[]}]}