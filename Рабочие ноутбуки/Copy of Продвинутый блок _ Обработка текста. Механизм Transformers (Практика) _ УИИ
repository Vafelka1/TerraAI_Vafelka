{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Продвинутый блок | Обработка текста. Механизм Transformers (Практика) | УИИ","provenance":[{"file_id":"1xxS3Ei1izxIexApV4Zq7w-AhxmjaPfrn","timestamp":1641485527255}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4350e26fa7dc47a88ea295a90a6e6370":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e17adc79a81d4b87a0047b24c1d480ec","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_de8b91be3282451e84ffe168a680b4a9","IPY_MODEL_d8fc420af7e946759289367fd7eb4c44","IPY_MODEL_940d91c05d3d4740b8334903495835ca"]}},"e17adc79a81d4b87a0047b24c1d480ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de8b91be3282451e84ffe168a680b4a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9a6c88784374485c90238c7cb871dcbd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Dl Completed...: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fdc10bc3e6854ec3b5c505174f910551"}},"d8fc420af7e946759289367fd7eb4c44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1cf288dd26f049b195377b383849bdd0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f9884e249c54d5fa598f418d16f297d"}},"940d91c05d3d4740b8334903495835ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e8e85fed9e5c48b2af3e05ad9a5eeeab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:07&lt;00:00,  3.81s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11d04276386a4cbe8af1ea2f0c90c827"}},"9a6c88784374485c90238c7cb871dcbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fdc10bc3e6854ec3b5c505174f910551":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1cf288dd26f049b195377b383849bdd0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3f9884e249c54d5fa598f418d16f297d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8e85fed9e5c48b2af3e05ad9a5eeeab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"11d04276386a4cbe8af1ea2f0c90c827":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"87ae51378b944df49c759152d9f76ead":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4ded2ccf5e604a60a9aee26342a89d6c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_25a7b7d7369245c8829815d5005056b5","IPY_MODEL_6462d51d0d7b4904be2816a51d37b449","IPY_MODEL_68e8ad1233eb4fb996a23a30c7cc0590"]}},"4ded2ccf5e604a60a9aee26342a89d6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"25a7b7d7369245c8829815d5005056b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_414a1db92e5d4a45b0930397c91ba553","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Dl Size...: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11e9c78099214af49ba1838c3b2e65d4"}},"6462d51d0d7b4904be2816a51d37b449":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b0df5fea4b434327874114f93fa2ab18","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a43c258b938485e8361d309fce9af5e"}},"68e8ad1233eb4fb996a23a30c7cc0590":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be2785b051a34701afd8677fdf885553","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 124/124 [00:07&lt;00:00, 40.26 MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_de7b0045b0664e16bfbd87319436437a"}},"414a1db92e5d4a45b0930397c91ba553":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"11e9c78099214af49ba1838c3b2e65d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0df5fea4b434327874114f93fa2ab18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2a43c258b938485e8361d309fce9af5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be2785b051a34701afd8677fdf885553":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"de7b0045b0664e16bfbd87319436437a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eeed55e5db884684940f0598ceb32a67":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5bdf6fc01fc54c0b9bf9134d5d9da787","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_795895a3f9d9490cbd20aeced5f516b9","IPY_MODEL_e5f0968ae5454c72bd6cb76399ab238d","IPY_MODEL_99ba0089bebe4e41b144e253a1b2dc8e"]}},"5bdf6fc01fc54c0b9bf9134d5d9da787":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"795895a3f9d9490cbd20aeced5f516b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a9e28bde494b47eda7c9834abcdf5b5a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Extraction completed...: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8a3bbb933f3c46aba3dae32d79dd2038"}},"e5f0968ae5454c72bd6cb76399ab238d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7a4009d5345145dbbf8f712189653fbd","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c2c4222efa1405b9d114842c9d99827"}},"99ba0089bebe4e41b144e253a1b2dc8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6be1339122f6470fa9e9342cab67c611","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:07&lt;00:00,  7.51s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45da93fd9072430c8181e40e0ebf8d34"}},"a9e28bde494b47eda7c9834abcdf5b5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8a3bbb933f3c46aba3dae32d79dd2038":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a4009d5345145dbbf8f712189653fbd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1c2c4222efa1405b9d114842c9d99827":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6be1339122f6470fa9e9342cab67c611":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"45da93fd9072430c8181e40e0ebf8d34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"777b547eaeed4838bebc8dd804798be4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3edde6f7c08e49d982ed26e283972f2c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8fb6d77ecb6c49eeb8160fc46ae3cb5c","IPY_MODEL_6bf54fe4675c4ab1aba2b7addef9dbe7","IPY_MODEL_87b37dce171d44859598f8effa24787e"]}},"3edde6f7c08e49d982ed26e283972f2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8fb6d77ecb6c49eeb8160fc46ae3cb5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9f4028ed63c64e84af0663a3a8db05a8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f5212c19aa1f4bba849f970d943f84f1"}},"6bf54fe4675c4ab1aba2b7addef9dbe7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_830f4d711eae494ba2149738c2e2c402","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_502edd8748f54dbeae9487220a032f91"}},"87b37dce171d44859598f8effa24787e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_01479f2e2bf542ec89bb26ce59d1aae0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5714/0 [00:01&lt;00:00, 6949.40 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f13b416bc564c109ebc074a96520919"}},"9f4028ed63c64e84af0663a3a8db05a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f5212c19aa1f4bba849f970d943f84f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"830f4d711eae494ba2149738c2e2c402":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"502edd8748f54dbeae9487220a032f91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01479f2e2bf542ec89bb26ce59d1aae0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7f13b416bc564c109ebc074a96520919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t9tZQsIipkm0","executionInfo":{"status":"ok","timestamp":1641485515032,"user_tz":-180,"elapsed":280,"user":{"displayName":"Vafelka","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11276804272897708630"}},"outputId":"59377b5a-2907-400c-a5c4-57583379934a"},"source":["print('\\033[1;37;41m ВНИМАНИЕ! \\033[0;0m')\n","print('Необходимо начать повествование, поставить здачу на урок. А что мы тут делаем-то вообще?)')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;37;41m ВНИМАНИЕ! \u001b[0;0m\n","Необходимо начать повествование, поставить здачу на урок. А что мы тут делаем-то вообще?)\n"]}]},{"cell_type":"code","metadata":{"id":"pJDFVpe9rAYP","executionInfo":{"status":"ok","timestamp":1641485517330,"user_tz":-180,"elapsed":2302,"user":{"displayName":"Vafelka","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11276804272897708630"}}},"source":["# датасет от tensorfolw, перевод португальских фраз на английский\n","import tensorflow_datasets as tfds \n","\n","# tensorflow\n","import tensorflow as tf \n","\n","# модуль отвечает за время и обработку временнЫх переменных\n","import time \n","\n","# numpy\n","import numpy as np \n","\n","# для отрисовки картинок в колабе\n","import matplotlib.pyplot as plt "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233,"referenced_widgets":["4350e26fa7dc47a88ea295a90a6e6370","e17adc79a81d4b87a0047b24c1d480ec","de8b91be3282451e84ffe168a680b4a9","d8fc420af7e946759289367fd7eb4c44","940d91c05d3d4740b8334903495835ca","9a6c88784374485c90238c7cb871dcbd","fdc10bc3e6854ec3b5c505174f910551","1cf288dd26f049b195377b383849bdd0","3f9884e249c54d5fa598f418d16f297d","e8e85fed9e5c48b2af3e05ad9a5eeeab","11d04276386a4cbe8af1ea2f0c90c827","87ae51378b944df49c759152d9f76ead","4ded2ccf5e604a60a9aee26342a89d6c","25a7b7d7369245c8829815d5005056b5","6462d51d0d7b4904be2816a51d37b449","68e8ad1233eb4fb996a23a30c7cc0590","414a1db92e5d4a45b0930397c91ba553","11e9c78099214af49ba1838c3b2e65d4","b0df5fea4b434327874114f93fa2ab18","2a43c258b938485e8361d309fce9af5e","be2785b051a34701afd8677fdf885553","de7b0045b0664e16bfbd87319436437a","eeed55e5db884684940f0598ceb32a67","5bdf6fc01fc54c0b9bf9134d5d9da787","795895a3f9d9490cbd20aeced5f516b9","e5f0968ae5454c72bd6cb76399ab238d","99ba0089bebe4e41b144e253a1b2dc8e","a9e28bde494b47eda7c9834abcdf5b5a","8a3bbb933f3c46aba3dae32d79dd2038","7a4009d5345145dbbf8f712189653fbd","1c2c4222efa1405b9d114842c9d99827","6be1339122f6470fa9e9342cab67c611","45da93fd9072430c8181e40e0ebf8d34","777b547eaeed4838bebc8dd804798be4","3edde6f7c08e49d982ed26e283972f2c","8fb6d77ecb6c49eeb8160fc46ae3cb5c","6bf54fe4675c4ab1aba2b7addef9dbe7","87b37dce171d44859598f8effa24787e","9f4028ed63c64e84af0663a3a8db05a8","f5212c19aa1f4bba849f970d943f84f1","830f4d711eae494ba2149738c2e2c402","502edd8748f54dbeae9487220a032f91","01479f2e2bf542ec89bb26ce59d1aae0","7f13b416bc564c109ebc074a96520919"]},"id":"dhnpa99mrGQL","outputId":"718acb45-bb64-45c2-fa2b-7aea5749e69e"},"source":["examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', \n","                               with_info=True, \n","                               as_supervised=True)\n","\n","# разделяем на обучающую и тестовую выборки\n","train_examples, val_examples = examples['train'], examples['validation'] "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mDownloading and preparing dataset ted_hrlr_translate/ru_to_en/1.0.0 (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /root/tensorflow_datasets/ted_hrlr_translate/ru_to_en/1.0.0...\u001b[0m\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4350e26fa7dc47a88ea295a90a6e6370","version_minor":0,"version_major":2},"text/plain":["Dl Completed...: 0 url [00:00, ? url/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87ae51378b944df49c759152d9f76ead","version_minor":0,"version_major":2},"text/plain":["Dl Size...: 0 MiB [00:00, ? MiB/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eeed55e5db884684940f0598ceb32a67","version_minor":0,"version_major":2},"text/plain":["Extraction completed...: 0 file [00:00, ? file/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"777b547eaeed4838bebc8dd804798be4","version_minor":0,"version_major":2},"text/plain":["0 examples [00:00, ? examples/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"8Xf99UKtuVBa"},"source":["Взглянем на примеры:"]},{"cell_type":"code","metadata":{"id":"aEIyigMAuVbh"},"source":["[en.numpy() for pt, en in train_examples.take(10)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbi4CkWMt7cq"},"source":["Получим разные токенайзеры для фраз на разных языках, размер словаря не ограничен. \n","\n","Токенайзеры применены не обычные, т.к. они не просто индексируют слова, а еще делят слова на части, которые чаще всего встречаются и индексируют именно их."]},{"cell_type":"code","metadata":{"id":"3Vun0zZzdFon"},"source":["print('\\033[1;37;41m ВНИМАНИЕ! \\033[0;0m')\n","print('Необходимо добавить пояснение, что за вид токенайзера такой, ранее был рассмотрен только стандартный')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuMdZXj8rJMd"},"source":["tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n","\n","tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krEZ_dNIuM9u"},"source":["Проверим как работает токенайзер на примере одной фразы"]},{"cell_type":"code","metadata":{"id":"KFRgBQrNrLfr"},"source":["# тестовая фраза\n","sample_string = 'Transformer is complicated.' \n","\n","# перегоним слова в токены\n","tokenized_string = tokenizer_en.encode(sample_string) \n","print ('Tokenized string is {}'.format(tokenized_string))\n","\n","# перегоним токены обратно в слова\n","original_string = tokenizer_en.decode(tokenized_string) \n","print ('The original string: {}'.format(original_string))\n","\n","# распишем какой токен чему соответсвует\n","for ts in tokenized_string:\n","  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts]))) \n","\n","# а точно исходная фраза соответствует дважды перекодированной?\n","assert original_string == sample_string "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OzyLgPGiwWvd"},"source":["Создадим функцию, которая добавит токены `< start >` и `< end >` во все фразы в датасете. Это токены не 0 1 , здесь они равны размеры словаря, то есть больше на 1 и 2, чем последнее слово:  "]},{"cell_type":"markdown","metadata":{"id":"LYQlyIDpevYb"},"source":[""]},{"cell_type":"code","metadata":{"id":"HbWoj9ibevoT"},"source":["print('\\033[1;37;41m ВНИМАНИЕ! \\033[0;0m')\n","print('Необходимо добавить пояснение, как работает функция ниже')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7krXaWGrO2e"},"source":["def encode(lang1, lang2):\n","  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n","      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n","\n","  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n","      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n","  \n","  return lang1, lang2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0L_W02g_xOIH"},"source":["Обертка от tensorflow для питоновских функций - здесь для упрощения подготовки датасета"]},{"cell_type":"code","metadata":{"id":"Jil34XdZe2CX"},"source":["print('\\033[1;37;41m ВНИМАНИЕ! \\033[0;0m')\n","print('Возможно, обертка это вообще тема для отдельного ноута в базу знаний, но как минимум пару слов надо сказать, что это такое и как упрощает')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjhbAUSKrRND"},"source":["def tf_encode(pt, en):\n","  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n","  result_pt.set_shape([None])\n","  result_en.set_shape([None])\n","\n","  return result_pt, result_en"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgtx_YZRrUT0"},"source":["# размер буффеа в памяти при приготовления датасета\n","BUFFER_SIZE = 20000 \n","\n","# размер батча\n","BATCH_SIZE = 64  \n","\n","# ограничим максимальную длину фразу\n","MAX_LENGTH = 40  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpWmABe9nGCh"},"source":["print('\\033[1;37;41m ВНИМАНИЕ! \\033[0;0m')\n","print('Необходимо добавить пояснение, что за функция ниже')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kr17q_wOrZyU"},"source":["def filter_max_length(x, y, max_length=MAX_LENGTH):\n","    \n","  return tf.logical_and(tf.size(x) <= max_length, # вернем маску, она уберет впоследствии длииные фразы\n","                        tf.size(y) <= max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-UtcR-O2YFb"},"source":["Готовим датасет\n"]},{"cell_type":"code","metadata":{"id":"v_Zp7UhVrcCj"},"source":["# Прогоняем тектсы через токенайзер\n","train_dataset = train_examples.map(tf_encode) \n","\n","# Убираем длинные фразу\n","train_dataset = train_dataset.filter(filter_max_length) \n","\n","# cache the dataset to memory to get a speedup while reading from it.\n","\n","# Грузим в память\n","train_dataset = train_dataset.cache() \n","\n","# Делим на батчи и перемешиваем\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE) \n","\n","# Кешируем датасет\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE) \n","\n","val_dataset = val_examples.map(tf_encode)\n","val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eu9XDBbXrd_f"},"source":["pt_batch, en_batch = next(iter(val_dataset))\n","\n","pt_batch, en_batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gDH0SGFV33XD"},"source":["Функция для расчета аргументы sin и cos для позиционного кодирования"]},{"cell_type":"code","metadata":{"id":"JrZ906UOrgFh"},"source":["def get_angles(pos, i, d_model):\n","    \n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TbkM1qKY4BNA"},"source":["Функция для расчета векторов позиционного кодирования"]},{"cell_type":"code","metadata":{"id":"e0nZIgp_fUyp"},"source":["print('\\033[1;37;41m ВНИМАНИЕ! \\033[0;0m')\n","print('Необходим перевод и пояснение, как работает функция ниже, что принимает')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRNOsOpQriI1"},"source":["def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","  \n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  \n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    \n","  pos_encoding = angle_rads[np.newaxis, ...]\n","    \n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9K_R_xKv4QqM"},"source":["Считаем вектора поцизионного кодирования"]},{"cell_type":"code","metadata":{"id":"U1j5IJngrkVY"},"source":["pos_encoding = positional_encoding(50, 512)\n","print (pos_encoding.shape)\n","\n","plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, 512))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hShhhSaU4Wwk"},"source":["Пишем класс MultiHeadAttention. Писать слои attention своими руками не будем, используем готовый слой Attention из модуля keras (`tf.keras.layers.Attention(use_scale=False, **kwargs))`\n","\n","Принимает на вход список из наших знакомых `Query, Value, Key`\n","На выходе вектор внимания той же размерности.\n","\n"]},{"cell_type":"code","metadata":{"id":"qjuiqQf7rvi7"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model = 512, num_heads = 8, causal=False, dropout=0.0):\n","\n","    super(MultiHeadAttention, self).__init__()  # здесь все головы считаем паралелльно - поэтому attention пробегает и  по батчу и по числу голов\n","\n","    assert d_model % num_heads == 0 # проверим, что размерность головы - целое цисло\n","    depth = d_model // num_heads    # размерность каждой головы внимания  =>>  multi-headed_depth = depth / num_heads\n","                                    # заметим, что выходы голов конкатенируются, поэтому их размер в их число раз меньше.\n","                                    # поэтому размерность выхода должно совпасть в размерностью входа\n","\n","    self.w_query = tf.keras.layers.Dense(d_model)                            # dense для Query  (batch_size , seq_length, depth) \n","    self.split_reshape_query = tf.keras.layers.Reshape((-1,num_heads,depth)) # слой для решейпа query до  (batch_size , seq_length, num_heads, multi_headed_depth)  \n","    self.split_permute_query = tf.keras.layers.Permute((2,1,3))              # слой для перестановки размерностей для использования Attention (batch_size , num_heads, seq_length, multi_headed_depth) \n","\n","    self.w_value = tf.keras.layers.Dense(d_model)\n","    self.split_reshape_value = tf.keras.layers.Reshape((-1,num_heads,depth))\n","    self.split_permute_value = tf.keras.layers.Permute((2,1,3))\n","\n","    self.w_key = tf.keras.layers.Dense(d_model)\n","    self.split_reshape_key = tf.keras.layers.Reshape((-1,num_heads,depth))\n","    self.split_permute_key = tf.keras.layers.Permute((2,1,3))\n","\n","    self.attention = tf.keras.layers.Attention(causal=causal, dropout=dropout) # прописывает слой Attention\n","    self.join_permute_attention = tf.keras.layers.Permute((2,1,3))             # слой обратной перестановки размерностей\n","    self.join_reshape_attention = tf.keras.layers.Reshape((-1,d_model))        # слой обратного решейпа до размернсти модели\n","\n","    self.dense = tf.keras.layers.Dense(d_model)\n","\n","  def call(self, inputs, mask=None, training=None): # основная рабочая функция\n","\n","    q = inputs[0]                           # вытащим входной вектор Query\n","    v = inputs[1]                           # вытащим входной вектор Value\n","    k = inputs[2] if len(inputs) > 2 else v # вытащим входной вектор Key\n","\n","    query = self.w_query(q)                   # прогоним через dense\n","    query = self.split_reshape_query(query)   # решейпим\n","    query = self.split_permute_query(query)   # переставляем размерность\n","\n","    value = self.w_value(v)                 # прогоним через dense\n","    value = self.split_reshape_value(value) # решейпим\n","    value = self.split_permute_value(value) # переставляем размерность\n","\n","    key = self.w_key(k)               # прогоним через dense\n","    key = self.split_reshape_key(key) # решейпим\n","    key = self.split_permute_key(key) # переставляем размерность\n","\n","    if mask is not None: # применим маску если есть \n","\n","      if mask[0] is not None:\n","        mask[0] = tf.keras.layers.Reshape((-1,1))(mask[0])\n","        mask[0] = tf.keras.layers.Permute((2,1))(mask[0])\n","      \n","      if mask[1] is not None:\n","        mask[1] = tf.keras.layers.Reshape((-1,1))(mask[1])\n","        mask[1] = tf.keras.layers.Permute((2,1))(mask[1])\n","\n","    attention = self.attention([query, value, key], mask=mask) # вычисляем векторо внимания\n","    attention = self.join_permute_attention(attention)         # обратно перемешиваем размерности\n","    attention = self.join_reshape_attention(attention)         # решейпим до (batch_size , seq_length, depth) \n","\n","    x = self.dense(attention) # выходной dense\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mAeeSGSOWXxE"},"source":["Определим класс кодера. Он содержит один слой \"многоголовое внимание\", дропаут, слой сложения - создает resudial связь, слой нормализации и Feed Forward блок - в dense слоя."]},{"cell_type":"code","metadata":{"id":"I2EGJ0VHr9Tg"},"source":["class EncoderLayer(tf.keras.layers.Layer): \n","\n","  def __init__(self,  d_model = 512, num_heads = 8, dff = 2048, dropout = 0.0):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.multi_head_attention =  MultiHeadAttention(d_model, num_heads)  # определим слой  MultiHeadAttention\n","    self.dropout_attention = tf.keras.layers.Dropout(dropout)            # добавим дропаут\n","    self.add_attention = tf.keras.layers.Add()                           # слой для получения resudial связи\n","    self.layer_norm_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)  # слой нормализации\n","\n","    self.dense1 = tf.keras.layers.Dense(dff, activation='relu')               # первый dense в Feed Forward\n","    self.dense2 = tf.keras.layers.Dense(d_model)                              # второй  dense в Feed Forward\n","    self.dropout_dense = tf.keras.layers.Dropout(dropout)                     # добавим дропаут\n","    self.add_dense = tf.keras.layers.Add()                                    # слой для получения resudial связи\n","    self.layer_norm_dense = tf.keras.layers.LayerNormalization(epsilon=1e-6)  # слой нормализации\n","\n","  def call(self, inputs, mask=None, training=None):\n","    \n","    # print(mask)\n","\n","    # входной вектор input размножим до 3  - это query, value, key и отдадим attenton, с маской если есть \n","    attention = self.multi_head_attention([inputs,inputs,inputs], mask = [mask,mask]) \n","    \n","    # уменьшим переобучение дроаутом\n","    attention = self.dropout_attention(attention, training = training) \n","    \n","    # сделаем resudial связь - добавим входной вектор\n","    x = self.add_attention([inputs , attention]) \n","    \n","    # далее нормализация в масштабах уровня\n","    x = self.layer_norm_attention(x)  \n","    # x = inputs\n","\n","    ## добавим Feed Forward  \n","    \n","    # проходим 1й dense с активашкой relu\n","    dense = self.dense1(x) \n","    \n","    # второй dense без активашки т.е. с линейной активационной функцией\n","    dense = self.dense2(dense)  \n","    \n","    # добавляем дропаут\n","    dense = self.dropout_dense(dense, training = training)  \n","    \n","    # еще одна resudial связь\n","    x = self.add_dense([x , dense])  \n","    \n","    # нормализация в масштабах уровня\n","    x = self.layer_norm_dense(x)  \n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xru8wFSFafUj"},"source":["Определим класс слой декодера, который содержит один слой \"многоголовое внимание\", дропаут, слой сложения - создает resudial связь, слой нормализации и Feed Forward блок - в dense слоя:"]},{"cell_type":"code","metadata":{"id":"5Eyo1MWfsBPU"},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","\n","  def __init__(self,  d_model = 512, num_heads = 8, dff = 2048, dropout = 0.0):\n","\n","    super(DecoderLayer, self).__init__()\n","\n","    self.multi_head_attention1 =  MultiHeadAttention(d_model, num_heads, causal = True) # определим слой  MultiHeadAttention\n","    self.dropout_attention1 = tf.keras.layers.Dropout(dropout)                          # добавим дропаут\n","    self.add_attention1 = tf.keras.layers.Add()                                         # слой для получения resudial связи\n","    self.layer_norm_attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)       # слой нормализации\n","\n","    self.multi_head_attention2 =  MultiHeadAttention(d_model, num_heads)                # определим слой  MultiHeadAttention \n","    self.dropout_attention2 = tf.keras.layers.Dropout(dropout)                          # добавим дропаут\n","    self.add_attention2 = tf.keras.layers.Add()                                         # слой для получения resudial связи\n","    self.layer_norm_attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)       # слой нормализации\n","\n","\n","    self.dense1 = tf.keras.layers.Dense(dff, activation='relu')              # первый dense в Feed Forward\n","    self.dense2 = tf.keras.layers.Dense(d_model)                             # второй  dense в Feed Forward\n","    self.dropout_dense = tf.keras.layers.Dropout(dropout)                    # добавим дропаут\n","    self.add_dense = tf.keras.layers.Add()                                   # слой для получения resudial связи\n","    self.layer_norm_dense = tf.keras.layers.LayerNormalization(epsilon=1e-6) # слой нормализации\n","\n","  def call(self, inputs, mask=None, training=None):\n","    \n","    # print(mask)\n","    \n","    # подадим на multi-head attention  Query, Key, Value вектора - здесь это один и тотже вектор\n","    attention = self.multi_head_attention1([inputs[0],inputs[0],inputs[0]], mask = [mask[0],mask[0]])\n","    \n","    # пробежим дропаут\n","    attention = self.dropout_attention1(attention, training = training)\n","    \n","    # пробросим Resudial-связь\n","    x = self.add_attention1([inputs[0] , attention])\n","    \n","    # выполним нормализацию\n","    x = self.layer_norm_attention1(x)\n","    \n","    # второй multi-head attention. Подаем выход предыдущих слоев x и выход от энкодера, маску (если есть)\n","    attention = self.multi_head_attention2([x, inputs[1],inputs[1]], mask = [mask[0],mask[1]])\n","    \n","    # прогоняем через дропаут\n","    attention = self.dropout_attention2(attention, training = training)\n","    \n","    # пробросим Resudial-связь\n","    x = self.add_attention1([x , attention])\n","    \n","    # выполним нормализацию\n","    x = self.layer_norm_attention1(x)\n","\n","    ## Feed Forward\n","    \n","    # 1 -dense + relu\n","    dense = self.dense1(x)\n","\n","    # 2 dense +linear\n","    dense = self.dense2(dense)\n","\n","    # прогоняем через дропаут\n","    dense = self.dropout_dense(dense, training = training)\n","\n","    # пробросим Resudial-связь\n","    x = self.add_dense([x , dense])\n","\n","    # выполним нормализацию\n","    x = self.layer_norm_dense(x)\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6GY0fktccgh"},"source":["Создаем сам кодер из слоев, описанных ранее (embedding, позиционное кодирование, пачки кодеров):\n"]},{"cell_type":"code","metadata":{"id":"DSMPCIhfsD-C"},"source":["class Encoder(tf.keras.layers.Layer):\n","\n","  def __init__(self, input_vocab_size, num_layers = 4, d_model = 512, num_heads = 8, dff = 2048, maximum_position_encoding = 10000, dropout = 0.0):\n","    \n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","\n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model, mask_zero=True)                           # пропишем слой Embedding\n","    self.pos = positional_encoding(maximum_position_encoding, d_model)                                              # функция позиционного кодирования\n","    self.encoder_layers = [ EncoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout) for _ in range(num_layers)] # список Nx слоев кодера \n","    self.dropout = tf.keras.layers.Dropout(dropout)                                                                 # просто дропаут\n","\n","  def call(self, inputs, mask=None, training=None):\n","    \n","    # входной эмбеддинг  \n","    x = self.embedding(inputs) \n","    \n","    # позиционное кодирование\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  #\n","    x += self.pos[: , :tf.shape(x)[1], :]\n","    \n","    # прогоняем через дроаут\n","    x = self.dropout(x, training=training)\n","\n","    # Encoder layer\n","\n","    # входной слой embedding+ Позиционное кодирование\n","    embedding_mask = self.embedding.compute_mask(inputs)\n","\n","    # прогоним через Nx слоев кодера\n","    for encoder_layer in self.encoder_layers: \n","      x = encoder_layer(x, mask = embedding_mask)\n","\n","    return x\n","\n","  # Закомментируйте код ниже, если хотите использовать masked_loss()\n","  def compute_mask(self, inputs, mask=None):\n","    return self.embedding.compute_mask(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tWn5jlsVum7Y"},"source":["Создаем сам декодер из слоев, описанных выше (embedding, позиционное кодирование, пачки декодеров):"]},{"cell_type":"code","metadata":{"id":"AoyyU00JsPaH"},"source":["class Decoder(tf.keras.layers.Layer):\n","\n","  def __init__(self, target_vocab_size, num_layers = 4, d_model = 512, num_heads = 8, dff = 2048, maximum_position_encoding = 10000, dropout = 0.0):\n","    \n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","\n","    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model, mask_zero=True) # пропишем слой Embedding\n","    self.pos = positional_encoding(maximum_position_encoding, d_model)                     # функция позиционного кодирования\n","    self.decoder_layers = [ DecoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout)  for _ in range(num_layers)] # список Nx слоев декодера \n","    self.dropout = tf.keras.layers.Dropout(dropout)                                        # просто дропаут\n","\n","  def call(self, inputs, mask=None, training=None):\n","    \n","    # выполним переход в эмбеддинг\n","    x = self.embedding(inputs[0]) \n","    \n","    # позиционное кодирование\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos[: , :tf.shape(x)[1], :]\n","    \n","    # просто dropout\n","    x = self.dropout(x, training=training)\n","\n","    # Decoder layer\n","    \n","    # входной слой embedding + Позиционное кодирование\n","    embedding_mask = self.embedding.compute_mask(inputs[0])\n","\n","    # прогоним через Nx слоев декодера\n","    for decoder_layer in self.decoder_layers:\n","      x = decoder_layer([x,inputs[1]], mask = [embedding_mask, mask])\n","\n","    return x\n","\n","  # Закомментируйте код ниже, если хотите использовать masked_loss()\n","  def compute_mask(self, inputs, mask=None):\n","    return self.embedding.compute_mask(inputs[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZvy540-fE2C"},"source":["Соберем модель: 2 входных слоя, кодер, декодер, \n","выходной dense:"]},{"cell_type":"code","metadata":{"id":"l2UU7F63sR2W"},"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","dropout_rate = 0.1\n","\n","input_vocab_size = tokenizer_pt.vocab_size + 2\n","target_vocab_size = tokenizer_en.vocab_size + 2\n","\n","\n","input  = tf.keras.layers.Input(shape=(None,)) # входной слой для переводимой фразы\n","target = tf.keras.layers.Input(shape=(None,)) # вход для перевода\n","encoder = Encoder(input_vocab_size, num_layers = num_layers, d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout_rate) # весь кодер\n","decoder = Decoder(target_vocab_size, num_layers = num_layers, d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout_rate) #весь декодер\n","\n","# собираем модель \n","x = encoder(input)\n","x = decoder([target, x] , mask = encoder.compute_mask(input))\n","x = tf.keras.layers.Dense(target_vocab_size)(x)\n","\n","model = tf.keras.models.Model(inputs=[input, target], outputs=x)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bNdbJ45Wfd2R"},"source":["Проверим работоспособность:"]},{"cell_type":"code","metadata":{"id":"IfBWHYgtsVK4"},"source":["pt_batch, en_batch = next(iter(val_dataset))\n","\n","plt.pcolormesh(model.predict([pt_batch,en_batch])[5],cmap='RdBu')\n","plt.colorbar()\n","\n","model.predict([pt_batch,en_batch]).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxmqN62Of6l7"},"source":["print('\\033[1;37;41m ВНИМАНИЕ! \\033[0;0m')\n","print('Необходимо добавить пояснение, как устроен класс ниже')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p80e0Wu8sYE6"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    \n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0rfkHGFofm5I"},"source":["Задаем оптимайзер, функцию потерь, метрики и компилируем модель"]},{"cell_type":"code","metadata":{"id":"lEw7v5insgBS"},"source":["# оптимайзер \n","optimizer = tf.keras.optimizers.Adam(CustomSchedule(d_model), beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)\n","# функция потерь\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def masked_loss(y_true, y_pred):\n","\n","  # считаем маску  \n","  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n","  _loss = loss(y_true, y_pred)\n","\n","  mask = tf.cast(mask, dtype=_loss.dtype)\n","\n","  # накинем маску на потери  \n","  _loss *= mask\n","\n","  return tf.reduce_sum(_loss)/tf.reduce_sum(mask)\n","\n","metrics = [loss, masked_loss, tf.keras.metrics.SparseCategoricalAccuracy()]\n","\n","model.compile(optimizer=optimizer, loss = loss, metrics = metrics) # masked_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3P24jwCfwgY"},"source":["Проверим число батчей для обучения и проверки:"]},{"cell_type":"code","metadata":{"id":"1LqO3IhAsiUh"},"source":["num_batches = 0\n","for (batch, (_,_)) in enumerate(train_dataset):\n","  num_batches = batch\n","print(num_batches)\n","\n","val_batches = 0\n","for (batch, (_,_)) in enumerate(val_dataset):\n","  val_batches = batch\n","print(val_batches)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VoaphoWef-6l"},"source":["Напишем генератор. По номеру будем получать батч данных для обучения, запустим обучение:"]},{"cell_type":"code","metadata":{"id":"CvmyShZXnUOZ"},"source":["print('\\033[1;37;41m ВНИМАНИЕ! \\033[0;0m')\n","print('Необходимо добавить пояснение, что за функция ниже')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoWgLVK-skQN"},"source":["def generator(data_set):\n","    \n","  while True:\n","    for pt_batch, en_batch in data_set:\n","      yield ( [pt_batch , en_batch[:, :-1] ] , en_batch[:, 1:] )\n","\n","history = model.fit(x = generator(train_dataset), validation_data = generator(val_dataset), epochs=20, steps_per_epoch = num_batches, validation_steps = val_batches)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EgdKVZLgby1"},"source":["## Prediction\n","На кодер подает исходную фразу, токен start на декодер - предиктим первое слово.\n","Выход декорера подаем на его вход и предиктим второе слово. \n","Второе слово дописываем к первому, и подаем на вход декодера\n","Процедура останавливается, как только токен stop получен\n","Переведенная фраза без токенов start/stop декодируется в текст.\n","\n","Проверим качество перевода. Выведем вместе предикченных перевод и эталонную фразу"]},{"cell_type":"code","metadata":{"id":"HLRzE3eqsqy9"},"source":["for i in range(10):\n","  translation = [tokenizer_en.vocab_size]\n","  for _ in range(40):\n","    predict = model.predict([pt_batch[i:i+1],np.asarray([translation])])\n","    translation.append(np.argmax(predict[-1,-1]))\n","    if translation[-1] == tokenizer_en.vocab_size + 1:\n","      break\n","\n","  real_translation = []\n","  for w in en_batch[:,1:][i].numpy():\n","    if w == tokenizer_en.vocab_size + 1:\n","      break\n","    real_translation.append(w)\n","  print(tokenizer_en.decode(real_translation))\n","  print(tokenizer_en.decode(translation[1:-1]))\n","  print(\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_HIKiO0iiUf"},"source":["Почитаем фразы - смысл уже можно уловить, но хотелось бы лучше..."]}]}